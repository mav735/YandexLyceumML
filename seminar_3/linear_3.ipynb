{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание - линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с признаками (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте датасет из материалов к уроку или по ссылке https://raw.githubusercontent.com/jupiterzhuo/travel-insurance/master/travel%20insurance.csv \n",
    "\n",
    "\n",
    "Описание признаков:\n",
    "\n",
    "* Agency — название страхового агентства\n",
    "* Agency Type — тип страхового агентства\n",
    "* Distribution Channel — канал продвижения страхового агентства\n",
    "* Product Name — название страхового продукта\n",
    "* Duration — длительность поездки (количество дней)\n",
    "* Destination — направление поездки\n",
    "* Net Sales — сумма продаж \n",
    "* Commission (in value) — комиссия страхового агентства\n",
    "* Gender — пол застрахованного\n",
    "* Age — возраст застрахованного\n",
    "\n",
    "Ответ:\n",
    "* Claim — потребовалась ли страховая выплата: «да» — 1, «нет» — 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработайте пропущенные значения и примените написанные функции onehot_encode() и minmax_scale().\n",
    "\n",
    "**Подсказка**: маску для категориальных признаков можно сделать фильтром cat_features_mask = (df.dtypes == \"object\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Duration  Net Sales  Commision (in value)       Age  Agency1  Agency2  \\\n",
      "0      0.038501   0.300250              0.033757  0.686441        0        0   \n",
      "1      0.038501   0.300250              0.033757  0.601695        0        0   \n",
      "2      0.013721   0.283153              0.104762  0.271186        0        0   \n",
      "3      0.012697   0.291410              0.083810  0.271186        0        0   \n",
      "4      0.016588   0.307923              0.041905  0.347458        0        0   \n",
      "...         ...        ...                   ...       ...      ...      ...   \n",
      "63321  0.023142   0.353628              0.043210  0.262712        0        0   \n",
      "63322  0.012288   0.357798              0.049383  0.338983        0        0   \n",
      "63323  0.000819   0.339450              0.022222  0.483051        0        0   \n",
      "63324  0.001024   0.339450              0.022222  0.533898        0        0   \n",
      "63325  0.004915   0.346122              0.032099  0.296610        0        0   \n",
      "\n",
      "       Agency3  Agency4  Agency5  Agency6  ...  Destination143  \\\n",
      "0            0        1        0        0  ...               0   \n",
      "1            0        1        0        0  ...               0   \n",
      "2            0        0        0        0  ...               0   \n",
      "3            0        0        0        0  ...               0   \n",
      "4            0        0        0        0  ...               0   \n",
      "...        ...      ...      ...      ...  ...             ...   \n",
      "63321        0        0        0        0  ...               0   \n",
      "63322        0        0        0        0  ...               0   \n",
      "63323        0        0        0        0  ...               0   \n",
      "63324        0        0        0        0  ...               0   \n",
      "63325        0        0        0        0  ...               0   \n",
      "\n",
      "       Destination144  Destination145  Destination146  Destination147  \\\n",
      "0                   0               0               0               0   \n",
      "1                   0               0               0               0   \n",
      "2                   0               0               0               0   \n",
      "3                   0               0               0               0   \n",
      "4                   0               0               0               0   \n",
      "...               ...             ...             ...             ...   \n",
      "63321               0               0               0               0   \n",
      "63322               0               0               0               0   \n",
      "63323               0               0               0               0   \n",
      "63324               0               0               1               0   \n",
      "63325               0               0               0               0   \n",
      "\n",
      "       Destination148  Destination149  Gender1  Gender2  Gender3  \n",
      "0                   0               0        0        1        0  \n",
      "1                   0               0        0        1        0  \n",
      "2                   0               0        1        0        0  \n",
      "3                   0               0        1        0        0  \n",
      "4                   0               0        1        0        0  \n",
      "...               ...             ...      ...      ...      ...  \n",
      "63321               0               0        0        0        1  \n",
      "63322               0               0        0        1        0  \n",
      "63323               0               0        0        0        1  \n",
      "63324               0               0        0        0        1  \n",
      "63325               0               0        0        1        0  \n",
      "\n",
      "[63326 rows x 202 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def minmax_scale(matrix: np.array, f_range=(0, 1)) -> np.array:\n",
    "    axis = 0 if len(matrix) > 1 else None\n",
    "    d_part = matrix.max(axis=axis) - matrix.min(axis=axis)\n",
    "    if matrix.size == 1:\n",
    "        d_part = 1\n",
    "    matrix_std = (matrix - matrix.min(axis=axis)) / d_part\n",
    "\n",
    "    matrix_scaled = matrix_std * (f_range[1] - f_range[0]) + f_range[0]\n",
    "    return matrix_scaled\n",
    "\n",
    "\n",
    "def onehot_encoding(matrix: np.array) -> np.array:\n",
    "    result = np.zeros([len(matrix), len(set(matrix))], dtype=int)\n",
    "    dictionary_values = {index: value for value, index in enumerate(sorted(set(matrix)))}\n",
    "    for value, index in enumerate(matrix):\n",
    "        result[value][dictionary_values[index]] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv('travel insurance.csv')\n",
    "cat_features_mask = (dataframe.dtypes == \"object\").values\n",
    "replacer = SimpleImputer(strategy=\"mean\")\n",
    "scaled_data = minmax_scale(pd.DataFrame(data=replacer.fit_transform(dataframe[dataframe.columns[~cat_features_mask]]),\n",
    "                                       columns=dataframe[dataframe.columns[~cat_features_mask]].columns))\n",
    "result_dict = {}\n",
    "for feature in dataframe[dataframe.columns[cat_features_mask]].fillna(\"\"):\n",
    "    if feature != 'Claim':\n",
    "        for value, index in enumerate(onehot_encoding(dataframe[dataframe.columns[cat_features_mask]].fillna(\"\")[feature]).T):\n",
    "            result_dict[feature + str(value + 1)] = index\n",
    "\n",
    "cleared_data = pd.concat([scaled_data, pd.DataFrame(result_dict)], axis=1)\n",
    "print(cleared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подробный анализ и подготовка датасета часто помогают улучшить качество модели. Ниже представлено несколько идей преобразований. Вы можете применить одно или несколько из этих преобразований (а можете не применять), чтобы помочь будущей модели. \n",
    "\n",
    "1. Посмотрите на количественные признаки. Возможно, в некоторых признаках есть выбросы - значения, которые сильно выбиваются. Такие значения полезно удалять. Советуем присмотреться к колонке Duration)\n",
    "\n",
    "2. Можно заметить, что one hot encoding сильно раздувает количество столбцов. Радикальное решение - можно попробовать выбросить все категориальные признаки из датасета.\n",
    "\n",
    "3. Если все-таки оставляете категориальные признаки, то подумайте, как уменьшить количество столбцов после one hot encoding. Признаки с большим количеством значений (Duration - 149! разных стран) можно удалить или попробовать сгруппировать некоторые значения.\n",
    "\n",
    "4. Downsampling. Датасет достаточно большой, разница в классах огромная. Можно уменьшить число наблюдений с частым ответом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_data = scaled_data # Использование только не категориальных фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение линейной регрессии (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это задача классификации, но её можно решить с помощью линейной регрессии, если округлять предсказанный ответ до целого и выбирать ближайший по значению ответ из множества {0, 1}.\n",
    "\n",
    "Вынесите признак 'Claim' в вектор ответов и разделите датасет на обучающую и тестовую выборку в соотношении 80 к 20. Зафиксируйте random_state.\n",
    "\n",
    "**Подсказка:** быстро перевести Yes/No в 1/0 можно так - np.where(df['Claim'] == 'Yes', 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(cleared_data),\n",
    "                                                    np.where(dataframe[dataframe.columns[cat_features_mask]].fillna(\"\")['Claim'] == 'Yes',\n",
    "                                                            1, \n",
    "                                                            0),\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=143)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите аналитическое решение для обучающей выборки: обычное и регуляризацией l2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10402076  0.08705588  0.33922122  0.03308683 -0.01465457]\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = np.hstack((np.ones((np.array(X_train.shape)[0], 1)), X_train))\n",
    "analytic_solved = np.dot(np.dot(np.linalg.inv(X_train_2.T @ X_train_2), X_train_2.T), y_train)\n",
    "print(analytic_solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10148202  0.08608959  0.33202727  0.03577968 -0.01477802]\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = np.hstack((np.ones((np.array(X_train.shape)[0], 1)), X_train))\n",
    "lambda_ = 1\n",
    "analytic_solved_2 = np.linalg.inv(X_train_2.T @ X_train_2 + lambda_ * lambda_ * np.eye(k + 1)) @ X_train_2.T @ y_train\n",
    "print(analytic_solved_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель LinearRegression, примените к тестовой выборке и посчитайте MSE (можно использовать библиотеку sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_linear_regressor = LinearRegression()\n",
    "sklearn_linear_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01417171759587896\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, sklearn_linear_regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод (1 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите краткий вывод по заданию (достаточно пары предложений). Расскажите, какие способы предобработки данных вы выбрали и почему. Насколько хороша ваша модель?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: При подготовке датасета я выбрал второй предложенный пример,\n",
    "так как мои предварительные исследования показали,\n",
    "что удаление всех категориальных признаков значительно снижает MSE.\n",
    "В этом датасете было огромное количество категориальных признаков, которые могут повлиять на правильную работу модели.\n",
    "Кроме того, я заметил, что аналитическое решение регрессии практически не отличается от решения с использованием регуляризации.\n",
    "\n",
    "Таким образом, мои результаты подтверждают важность правильной предобработки данных перед обучением модели и использования регуляризации для предотвращения переобучения. В будущем, я буду учитывать эти факторы при работе с подобными датасетами и принимать во внимание возможность удаления категориальных признаков для улучшения результатов модели."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3e645da81579eaba2c8f70e452043c00b8fbf6968663b443314327603f0fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
